{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression, Ridge\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from scipy.sparse import hstack\n",
    "from scipy.special import logit, expit\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "class_names = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "\n",
    "train = pd.read_csv('input/train.csv').fillna(' ')\n",
    "test = pd.read_csv('input/test.csv').fillna(' ')\n",
    "\n",
    "train_text = train['comment_text']\n",
    "test_text = test['comment_text']\n",
    "all_text = pd.concat([train_text, test_text])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing word vectorizer\n",
      "Doing char vectorizer\n",
      "Wall time: 10min 48s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "print (\"Doing word vectorizer\")\n",
    "word_vectorizer = TfidfVectorizer(\n",
    "            sublinear_tf=True,\n",
    "            strip_accents='unicode',\n",
    "            analyzer='word',\n",
    "            token_pattern=r'\\w{1,}',\n",
    "            ngram_range=(1, 1),\n",
    "            max_features=100000)\n",
    "\n",
    "word_vectorizer.fit(all_text)\n",
    "train_word_features = word_vectorizer.transform(train_text)\n",
    "test_word_features = word_vectorizer.transform(test_text)\n",
    "\n",
    "\n",
    "print (\"Doing char vectorizer\")\n",
    "char_vectorizer = TfidfVectorizer(\n",
    "            sublinear_tf=True,\n",
    "            strip_accents='unicode',\n",
    "            analyzer='char',\n",
    "            ngram_range=(1, 5),\n",
    "            max_features=100000)\n",
    "char_vectorizer.fit(all_text)\n",
    "\n",
    "train_char_features = char_vectorizer.transform(train_text)\n",
    "test_char_features = char_vectorizer.transform(test_text)\n",
    "\n",
    "train_features = hstack([train_char_features, train_word_features])\n",
    "test_features = hstack([test_char_features, test_word_features])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                            | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV score for class toxic is 0.9788316471982434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 17%|█████████████▊                                                                     | 1/6 [02:33<12:45, 153.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV score for class severe_toxic is 0.9886078733914744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 33%|███████████████████████████▋                                                       | 2/6 [05:14<10:28, 157.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV score for class obscene is 0.990175713630126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████████████████████████████████████████▌                                         | 3/6 [07:39<07:39, 153.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV score for class threat is 0.9889047468902628\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 67%|███████████████████████████████████████████████████████▎                           | 4/6 [10:52<05:26, 163.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV score for class insult is 0.9829270756824352\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 83%|█████████████████████████████████████████████████████████████████████▏             | 5/6 [13:30<02:42, 162.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV score for class identity_hate is 0.9827410917839409\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 6/6 [16:11<00:00, 161.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total CV score is 0.9853646914294139\n",
      "Wall time: 16min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "losses = []\n",
    "predictions = {'id': test['id']}\n",
    "for class_name in tqdm(class_names):\n",
    "    train_target = train[class_name]\n",
    "    classifier = LogisticRegression(solver='sag')\n",
    "\n",
    "    cv_loss = np.mean(cross_val_score(classifier, train_features, train_target, cv=3, scoring='roc_auc'))\n",
    "    losses.append(cv_loss)\n",
    "    print('CV score for class {} is {}'.format(class_name, cv_loss))\n",
    "\n",
    "    classifier.fit(train_features, train_target)\n",
    "    predictions[class_name] = classifier.predict_proba(test_features)[:, 1]\n",
    "\n",
    "print('Total CV score is {}'.format(np.mean(losses)))\n",
    "\n",
    "submission = pd.DataFrame.from_dict(predictions)\n",
    "submission.to_csv('submissions/logistic_start.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "identity_hate\n",
      "0.950899192572 [ 0.94447788  0.94815301  0.96006669]\n"
     ]
    }
   ],
   "source": [
    "print (class_name)\n",
    "y = train[class_name]\n",
    "rdg = Ridge(alpha=1.0, fit_intercept=True)\n",
    "cv_score = cross_val_score(rdg, train_features, train_target, cv=3, scoring='roc_auc')\n",
    "\n",
    "print (np.mean(cv_score),cv_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
